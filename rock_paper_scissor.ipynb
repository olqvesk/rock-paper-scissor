{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYBMxhaq5NY8",
        "outputId": "e78d9965-f47a-4ab3-d528-05735526cd97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 1s 0us/step\n",
            "60000\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "\n",
        "print(tf.__version__)   # Tensorflow의 버전을 출력\n",
        "\n",
        "mnist = keras.datasets.mnist\n",
        "\n",
        "# MNIST 데이터를 로드. 다운로드하지 않았다면 다운로드까지 자동으로 진행됩니다.\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "print(len(x_train))  # x_train 배열의 크기를 출력"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(x_train[10], cmap=plt.cm.binary)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "NuXyEq7Q5vVZ",
        "outputId": "f80357b6-d2db-445d-edc9-085834c0935a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ct47VNBC6GY7",
        "outputId": "86a6b3b7-96d1-4d75-93bf-bb45b3bc3463"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28)\n",
            "(10000, 28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_norm, x_test_norm = x_train / 255.0, x_test / 255.0\n",
        "print('최소값: ', np.min(x_train_norm), '최대값 : ', np.max(x_train_norm))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3beeb_s68gk",
        "outputId": "3f2e3002-2084-46f0-c17b-8c68a1797a6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최소값:  0.0 최대값 :  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=keras.models.Sequential()\n",
        "model.add(keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(28,28,1)))\n",
        "model.add(keras.layers.MaxPool2D(2,2))\n",
        "model.add(keras.layers.Conv2D(32, (3,3), activation='relu'))\n",
        "model.add(keras.layers.MaxPooling2D((2,2)))\n",
        "model.add(keras.layers.Flatten())\n",
        "model.add(keras.layers.Dense(32, activation='relu'))\n",
        "model.add(keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "print('Model에 추가된 Layer 개수 : ', len(model.layers))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMDhdfV27-Of",
        "outputId": "b826415a-c1d3-4b35-a4b8-18211079d4fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model에 추가된 Layer 개수 :  7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Before Reshape - x_train_norm shape : {}\".format(x_train_norm.shape))\n",
        "print(\"Before Reshape - x_test_norm shape : {}\".format(x_test_norm.shape))\n",
        "\n",
        "x_train_reshaped = x_train_norm.reshape( -1, 28, 28, 1)\n",
        "x_test_reshaped = x_test_norm.reshape( -1, 28, 28, 1)\n",
        "# 데이터 개수에 -1을 쓰면 reshape시 자동 계산이 된다.\n",
        "\n",
        "print(\"After Reshape - x_train_reshaped shape : {}\".format(x_train_reshaped.shape))\n",
        "print(\"After Reshape - x_test_reshaped shape : {}\".format(x_test_reshaped.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNarNAV-8jda",
        "outputId": "538bfc6d-b5c3-4b39-ecb0-4ea3ccf4fdb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before Reshape - x_train_norm shape : (60000, 28, 28)\n",
            "Before Reshape - x_test_norm shape : (10000, 28, 28)\n",
            "After Reshape - x_train_reshaped shape : (60000, 28, 28, 1)\n",
            "After Reshape - x_test_reshaped shape : (10000, 28, 28, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "             loss='sparse_categorical_crossentropy',\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train_reshaped, y_train, epochs=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsX0eYp48ujE",
        "outputId": "8ba018ef-931d-4bcf-e14e-522dfe2d4522"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 0.0123 - accuracy: 0.9955\n",
            "Epoch 2/20\n",
            "1875/1875 [==============================] - 30s 16ms/step - loss: 0.0097 - accuracy: 0.9969\n",
            "Epoch 3/20\n",
            "1875/1875 [==============================] - 30s 16ms/step - loss: 0.0075 - accuracy: 0.9977\n",
            "Epoch 4/20\n",
            "1875/1875 [==============================] - 29s 15ms/step - loss: 0.0083 - accuracy: 0.9970\n",
            "Epoch 5/20\n",
            "1875/1875 [==============================] - 31s 17ms/step - loss: 0.0068 - accuracy: 0.9978\n",
            "Epoch 6/20\n",
            "1875/1875 [==============================] - 30s 16ms/step - loss: 0.0060 - accuracy: 0.9983\n",
            "Epoch 7/20\n",
            "1875/1875 [==============================] - 29s 15ms/step - loss: 0.0053 - accuracy: 0.9981\n",
            "Epoch 8/20\n",
            "1875/1875 [==============================] - 29s 15ms/step - loss: 0.0064 - accuracy: 0.9980\n",
            "Epoch 9/20\n",
            "1875/1875 [==============================] - 30s 16ms/step - loss: 0.0057 - accuracy: 0.9980\n",
            "Epoch 10/20\n",
            "1875/1875 [==============================] - 30s 16ms/step - loss: 0.0042 - accuracy: 0.9987\n",
            "Epoch 11/20\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 0.0050 - accuracy: 0.9984\n",
            "Epoch 12/20\n",
            "1875/1875 [==============================] - 30s 16ms/step - loss: 0.0044 - accuracy: 0.9986\n",
            "Epoch 13/20\n",
            "1875/1875 [==============================] - 29s 16ms/step - loss: 0.0037 - accuracy: 0.9987\n",
            "Epoch 14/20\n",
            "1875/1875 [==============================] - 31s 16ms/step - loss: 0.0044 - accuracy: 0.9986\n",
            "Epoch 15/20\n",
            "1875/1875 [==============================] - 29s 15ms/step - loss: 0.0023 - accuracy: 0.9993\n",
            "Epoch 16/20\n",
            "1875/1875 [==============================] - 29s 15ms/step - loss: 0.0047 - accuracy: 0.9984\n",
            "Epoch 17/20\n",
            "1875/1875 [==============================] - 30s 16ms/step - loss: 0.0036 - accuracy: 0.9987\n",
            "Epoch 18/20\n",
            "1875/1875 [==============================] - 29s 15ms/step - loss: 0.0031 - accuracy: 0.9989\n",
            "Epoch 19/20\n",
            "1875/1875 [==============================] - 32s 17ms/step - loss: 0.0040 - accuracy: 0.9988\n",
            "Epoch 20/20\n",
            "1875/1875 [==============================] - 29s 16ms/step - loss: 0.0030 - accuracy: 0.9991\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f55ea6c5630>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(x_test_reshaped, y_test, verbose=2)\n",
        "print(\"test_loss : {}\".format(test_loss))\n",
        "print(\"test_accuracy : {}\".format(test_accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jg7MOa7hBQb0",
        "outputId": "130cd2f4-b658-4be2-f570-e28242fe0966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 - 3s - loss: 0.0503 - accuracy: 0.9896 - 3s/epoch - 8ms/step\n",
            "test_loss : 0.050348542630672455\n",
            "test_accuracy : 0.9896000027656555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(x_test_reshaped, y_test, verbose=2)\n",
        "print(\"test_loss : {}\".format(test_loss))\n",
        "print(\"test_accuracy : {}\".format(test_accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXohI8f3-zOf",
        "outputId": "961ff3f9-46c8-4fc1-8e38-1971878351c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 - 3s - loss: 0.0422 - accuracy: 0.9869 - 3s/epoch - 8ms/step\n",
            "test_loss : 0.04220985993742943\n",
            "test_accuracy : 0.9868999719619751\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model  # TensorFlow is required for Keras to work\n",
        "from PIL import Image, ImageOps  # Install pillow instead of PIL\n",
        "import numpy as np\n",
        "\n",
        "# Disable scientific notation for clarity\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "# Load the model\n",
        "model = load_model(\"/content/keras_model.h5\", compile=False)\n",
        "\n",
        "# Load the labels\n",
        "class_names = open(\"/content/labels.txt\", \"r\").readlines()\n",
        "\n",
        "# Create the array of the right shape to feed into the keras model\n",
        "# The 'length' or number of images you can put into the array is\n",
        "# determined by the first position in the shape tuple, in this case 1\n",
        "data = np.ndarray(shape=(1, 224, 224, 3), dtype=np.float32)\n",
        "\n",
        "# Replace this with the path to your image\n",
        "image = Image.open(\"/content/0.jpg\").convert(\"RGB\")\n",
        "\n",
        "# resizing the image to be at least 224x224 and then cropping from the center\n",
        "size = (224, 224)\n",
        "image = ImageOps.fit(image, size, Image.Resampling.LANCZOS)\n",
        "\n",
        "# turn the image into a numpy array\n",
        "image_array = np.asarray(image)\n",
        "\n",
        "# Normalize the image\n",
        "normalized_image_array = (image_array.astype(np.float32) / 127.5) - 1\n",
        "\n",
        "# Load the image into the array\n",
        "data[0] = normalized_image_array\n",
        "\n",
        "# Predicts the model\n",
        "prediction = model.predict(data)\n",
        "index = np.argmax(prediction)\n",
        "class_name = class_names[index]\n",
        "confidence_score = prediction[0][index]\n",
        "\n",
        "# Print prediction and confidence score\n",
        "print(\"Class:\", class_name[2:], end=\"\")\n",
        "print(\"Confidence Score:\", confidence_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNmWeZSWDHA2",
        "outputId": "dece9035-939c-4767-bcb2-27a03d6d5f28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f55e115a8c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 912ms/step\n",
            "Class: paper\n",
            "Confidence Score: 0.99999666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model  # TensorFlow is required for Keras to work\n",
        "from PIL import Image, ImageOps  # Install pillow instead of PIL\n",
        "import numpy as np\n",
        "\n",
        "# Disable scientific notation for clarity\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "# Load the model\n",
        "model = load_model(\"/content/keras_model.h5\", compile=False)\n",
        "\n",
        "# Load the labels\n",
        "class_names = open(\"/content/labels.txt\", \"r\").readlines()\n",
        "\n",
        "# Create the array of the right shape to feed into the keras model\n",
        "# The 'length' or number of images you can put into the array is\n",
        "# determined by the first position in the shape tuple, in this case 1\n",
        "data = np.ndarray(shape=(1, 224, 224, 3), dtype=np.float32)\n",
        "\n",
        "# Replace this with the path to your image\n",
        "image = Image.open(\"/content/3.jpg\").convert(\"RGB\")\n",
        "\n",
        "# resizing the image to be at least 224x224 and then cropping from the center\n",
        "size = (224, 224)\n",
        "image = ImageOps.fit(image, size, Image.Resampling.LANCZOS)\n",
        "\n",
        "# turn the image into a numpy array\n",
        "image_array = np.asarray(image)\n",
        "\n",
        "# Normalize the image\n",
        "normalized_image_array = (image_array.astype(np.float32) / 127.5) - 1\n",
        "\n",
        "# Load the image into the array\n",
        "data[0] = normalized_image_array\n",
        "\n",
        "# Predicts the model\n",
        "prediction = model.predict(data)\n",
        "index = np.argmax(prediction)\n",
        "class_name = class_names[index]\n",
        "confidence_score = prediction[0][index]\n",
        "\n",
        "# Print prediction and confidence score\n",
        "print(\"Class:\", class_name[2:], end=\"\")\n",
        "print(\"Confidence Score:\", confidence_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHqoKLPRHVOi",
        "outputId": "309da60a-20fb-43b0-fe1c-b50fddebbdcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 908ms/step\n",
            "Class: paper\n",
            "Confidence Score: 0.9378835\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model  # TensorFlow is required for Keras to work\n",
        "import cv2  # Install opencv-python\n",
        "import numpy as np\n",
        "\n",
        "# Disable scientific notation for clarity\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "# Load the model\n",
        "model = load_model(\"/content/keras_model.h5\", compile=False)\n",
        "\n",
        "# Load the labels\n",
        "class_names = open(\"/content/labels.txt\", \"r\").readlines()\n",
        "\n",
        "# CAMERA can be 0 or 1 based on default camera of your computer\n",
        "camera = cv2.VideoCapture(0)\n",
        "\n",
        "while True:\n",
        "    # Grab the webcamera's image.\n",
        "    ret, image = camera.read()\n",
        "\n",
        "    print(image.shape)\n",
        "    camera.release() # rele\n",
        "\n",
        "\n",
        "    # Resize the raw image into (224-height,224-width) pixels\n",
        "    image = cv2.resize(image, (224, 224), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    # Show the image in a window\n",
        "    cv2.imshow(\"Webcam Image\", image)\n",
        "\n",
        "    # Make the image a numpy array and reshape it to the models input shape.\n",
        "    image = np.asarray(image, dtype=np.float32).reshape(1, 224, 224, 3)\n",
        "\n",
        "    # Normalize the image array\n",
        "    image = (image / 127.5) - 1\n",
        "\n",
        "    # Predicts the model\n",
        "    prediction = model.predict(image)\n",
        "    index = np.argmax(prediction)\n",
        "    class_name = class_names[index]\n",
        "    confidence_score = prediction[0][index]\n",
        "\n",
        "    # Print prediction and confidence score\n",
        "    print(\"Class:\", class_name[2:], end=\"\")\n",
        "    print(\"Confidence Score:\", str(np.round(confidence_score * 100))[:-2], \"%\")\n",
        "\n",
        "    # Listen to the keyboard for presses.\n",
        "    keyboard_input = cv2.waitKey(1)\n",
        "\n",
        "    # 27 is the ASCII for the esc key on your keyboard.\n",
        "    if keyboard_input == 27:\n",
        "        break\n",
        "\n",
        "camera.release()\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "Oyh9C00gHhDi",
        "outputId": "7218954e-0bc4-44f9-8d4d-21ecf83d8754"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'shape'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-72038dafb2dd>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mcamera\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# rele\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import glob\n",
        "\n",
        "# /content/rock\n",
        "def resize_images(img_path):\n",
        "\timages=glob.glob(\"/content/\"+img_path + \"/*.jpg\")\n",
        "\n",
        "\tprint(len(images), \" images to be resized.\")\n",
        "\n",
        "    # 파일마다 모두 28x28 사이즈로 바꾸어 저장합니다.\n",
        "\ttarget_size=(28,28)\n",
        "\tfor img in images:\n",
        "\t\told_img=Image.open(img)\n",
        "\t\tnew_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
        "\t\tnew_img.save(img, \"JPEG\")\n",
        "\n",
        "\tprint(len(images), \" images resized.\")\n",
        "\n",
        "# 가위 이미지가 저장된 디렉토리 아래의 모든 jpg 파일을 읽어들여서\n",
        "image_dir_path = \"scissor\"\n",
        "resize_images(image_dir_path)\n",
        "\n",
        "print(\"가위 이미지 resize 완료!\")"
      ],
      "metadata": {
        "id": "KB89ozKvMGyP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a905274c-39a0-4c99-a5f8-e66278e0112f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "255  images to be resized.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-91559aa8f0ba>:14: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
            "  new_img=old_img.resize(target_size,Image.ANTIALIAS)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "255  images resized.\n",
            "가위 이미지 resize 완료!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_dir_path = \"rock\"\n",
        "resize_images(image_dir_path)\n",
        "\n",
        "image_dir_path = \"paper\"\n",
        "resize_images(image_dir_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_Up7bqyPd6l",
        "outputId": "e33b3cad-adcc-4394-86a4-b09d9401758a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "219  images to be resized.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-91559aa8f0ba>:14: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
            "  new_img=old_img.resize(target_size,Image.ANTIALIAS)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "219  images resized.\n",
            "224  images to be resized.\n",
            "224  images resized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def load_data(img_path, number_of_data=698):  # 가위바위보 이미지 개수 총합에 주의하세요.\n",
        "    # 가위 : 0, 바위 : 1, 보 : 2\n",
        "    img_size=28\n",
        "    color=3\n",
        "    #이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역을 생성합니다.\n",
        "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
        "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
        "\n",
        "    idx=0\n",
        "    for file in glob.iglob(img_path+'/scissor/*.jpg'):\n",
        "        img = np.array(Image.open(file),dtype=np.int32)\n",
        "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
        "        labels[idx]=0   # 가위 : 0\n",
        "        idx=idx+1\n",
        "\n",
        "    for file in glob.iglob(img_path+'/rock/*.jpg'):\n",
        "        img = np.array(Image.open(file),dtype=np.int32)\n",
        "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
        "        labels[idx]=1   # 바위 : 1\n",
        "        idx=idx+1\n",
        "\n",
        "    for file in glob.iglob(img_path+'/paper/*.jpg'):\n",
        "        img = np.array(Image.open(file),dtype=np.int32)\n",
        "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
        "        labels[idx]=2   # 보 : 2\n",
        "        idx=idx+1\n",
        "\n",
        "    print(\"학습데이터(x_train)의 이미지 개수는\", idx,\"입니다.\")\n",
        "    return imgs, labels\n",
        "\n",
        "image_dir_path = \"/content/r_s_p\" #폴더명\n",
        "(x_train, y_train)=load_data(image_dir_path)\n",
        "x_train_norm = x_train/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
        "\n",
        "print(\"x_train shape: {}\".format(x_train.shape))\n",
        "print(\"y_train shape: {}\".format(y_train.shape))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYQ5jNc-PhFE",
        "outputId": "b721930f-f6b9-4e55-b881-4f0c3c3f21e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "학습데이터(x_train)의 이미지 개수는 698 입니다.\n",
            "x_train shape: (698, 28, 28, 3)\n",
            "y_train shape: (698,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(x_train[0])\n",
        "print('라벨 : ', y_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "66b1BX4G3LbE",
        "outputId": "f6b224c7-5e47-421a-e471-fcfdf8ebb914"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "라벨 :  0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "model=keras.models.Sequential()\n",
        "model.add(keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,3)))\n",
        "model.add(keras.layers.MaxPool2D(2,2))\n",
        "model.add(keras.layers.Conv2D(64, (3,3), activation='relu'))\n",
        "model.add(keras.layers.MaxPooling2D((2,2)))\n",
        "model.add(keras.layers.Flatten())\n",
        "model.add(keras.layers.Dense(128, activation='relu'))\n",
        "model.add(keras.layers.Dense(3, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZW51rku4q63",
        "outputId": "b1de67ab-7cef-4e01-d243-281ce8281f56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 26, 26, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 13, 13, 32)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 5, 5, 64)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1600)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               204928    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 3)                 387       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 224707 (877.76 KB)\n",
            "Trainable params: 224707 (877.76 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "             loss='sparse_categorical_crossentropy',\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train_norm, y_train, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZK2iFpp64zR0",
        "outputId": "69d49507-49a5-4f7b-81c7-2d707c9c16ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "22/22 [==============================] - 2s 26ms/step - loss: 1.2191 - accuracy: 0.4971\n",
            "Epoch 2/10\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.6521 - accuracy: 0.7407\n",
            "Epoch 3/10\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.3286 - accuracy: 0.9169\n",
            "Epoch 4/10\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.1697 - accuracy: 0.9556\n",
            "Epoch 5/10\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.0863 - accuracy: 0.9814\n",
            "Epoch 6/10\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.0487 - accuracy: 0.9943\n",
            "Epoch 7/10\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.0280 - accuracy: 0.9986\n",
            "Epoch 8/10\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.0199 - accuracy: 0.9971\n",
            "Epoch 9/10\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.0112 - accuracy: 0.9986\n",
            "Epoch 10/10\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.0063 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7835215e5c30>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_dir_path = \"content/r_s_p/test\"\n",
        "(x_test, y_test)=load_data(image_dir_path)\n",
        "x_test_norm = x_test/255.0\n",
        "\n",
        "print(\"x_test shape : {}\".format(x_test.shape))\n",
        "print(\"y_test shape : {}\".format(y_test.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Oa4tqWx5CKZ",
        "outputId": "ed5be639-5c0f-40a3-89f0-504245d1a355"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "학습데이터(x_train)의 이미지 개수는 0 입니다.\n",
            "x_test shape : (698, 28, 28, 3)\n",
            "y_test shape : (698,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(x_test_norm, y_test, verbose=2)\n",
        "print(\"test_loss : {}\".format(test_loss))\n",
        "print(\"test_accuracy : {}\".format(test_accuracy))"
      ],
      "metadata": {
        "id": "pBf4aSZl5LWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_result = model.predict(x_test_norm)\t# model이 추론한 확률값\n",
        "predicted_labels = np.argmax(predicted_result, axis=1)\n",
        "\n",
        "idx=600\t\t# 값을 변경해서 찾아보자\n",
        "print('model.predict() 결과 : ', predicted_result[idx])\n",
        "print('model이 추론한 가장 가능성이 높은 결과 : ', predicted_labels[idx])\n",
        "print('실제 데이터의 라벨 : ', y_test[idx])"
      ],
      "metadata": {
        "id": "wZkKSmpb5SfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(x_test[idx], cmap=plt.cm.binary)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yTKkmkmD5TSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename\n",
        "\n",
        "# Use the function\n",
        "from keras.models import load_model\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Load the model and labels\n",
        "model = load_model(\"/content/keras_model.h5\", compile=False)\n",
        "class_names = open(\"/content/labels.txt\", \"r\").readlines()\n",
        "\n",
        "# Capture a photo\n",
        "filename = take_photo()\n",
        "\n",
        "# Load the photo\n",
        "image = cv2.imread(filename)\n",
        "\n",
        "# Preprocess the image\n",
        "image = cv2.resize(image, (224, 224), interpolation=cv2.INTER_AREA)\n",
        "image = np.asarray(image, dtype=np.float32).reshape(1, 224, 224, 3)\n",
        "image = (image / 127.5) - 1\n",
        "\n",
        "# Make a prediction\n",
        "prediction = model.predict(image)\n",
        "index = np.argmax(prediction)\n",
        "class_name = class_names[index]\n",
        "confidence_score = prediction[0][index]\n",
        "\n",
        "# Print the result\n",
        "print(f\"Class: {class_name[2:].strip()}\\nConfidence Score: {np.round(confidence_score * 100, 2)}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "TQjY2cQ5nw63",
        "outputId": "b8cc7263-655b-4899-801f-a2417320a805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function takePhoto(quality) {\n",
              "      const div = document.createElement('div');\n",
              "      const capture = document.createElement('button');\n",
              "      capture.textContent = 'Capture';\n",
              "      div.appendChild(capture);\n",
              "\n",
              "      const video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
              "\n",
              "      document.body.appendChild(div);\n",
              "      div.appendChild(video);\n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      // Resize the output to fit the video element.\n",
              "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
              "\n",
              "      // Wait for Capture to be clicked.\n",
              "      await new Promise((resolve) => capture.onclick = resolve);\n",
              "\n",
              "      const canvas = document.createElement('canvas');\n",
              "      canvas.width = video.videoWidth;\n",
              "      canvas.height = video.videoHeight;\n",
              "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
              "      stream.getVideoTracks()[0].stop();\n",
              "      div.remove();\n",
              "      return canvas.toDataURL('image/jpeg', quality);\n",
              "    }\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 947ms/step\n",
            "Class: scissor\n",
            "Confidence Score: 97.62%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Disable scientific notation for clarity\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "# Load the model\n",
        "model = load_model(\"/content/keras_model.h5\", compile=False)\n",
        "\n",
        "# Load the labels\n",
        "class_names = open(\"/content/labels.txt\", \"r\").readlines()\n",
        "\n",
        "# Load the video file (replace 'path_to_your_video.mp4' with your video file path)\n",
        "video_path = \"/content/path_to_your_video.mp4\"\n",
        "video = cv2.VideoCapture(video_path)\n",
        "\n",
        "while video.isOpened():\n",
        "    ret, frame = video.read()\n",
        "\n",
        "    # If there's a frame to process\n",
        "    if ret:\n",
        "        # Resize the raw frame into (224-height,224-width) pixels\n",
        "        resized_frame = cv2.resize(frame, (224, 224), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "        # Make the frame a numpy array and reshape it to the model's input shape.\n",
        "        image_array = np.asarray(resized_frame, dtype=np.float32).reshape(1, 224, 224, 3)\n",
        "\n",
        "        # Normalize the image array\n",
        "        image_array = (image_array / 127.5) - 1\n",
        "\n",
        "        # Predict the model\n",
        "        prediction = model.predict(image_array)\n",
        "        index = np.argmax(prediction)\n",
        "        class_name = class_names[index].strip()\n",
        "        confidence_score = prediction[0][index]\n",
        "\n",
        "        # Print prediction and confidence score\n",
        "        print(\"Class:\", class_name, \"Confidence Score:\", np.round(confidence_score * 100, 2), \"%\")\n",
        "\n",
        "        # Optionally, display the frame (disabled in Colab, but code included for completeness)\n",
        "        # cv2.imshow(\"Frame\", resized_frame)\n",
        "\n",
        "        # Listen to the keyboard for a break (disabled in Colab, but code included for completeness)\n",
        "        # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        #     break\n",
        "    else:\n",
        "        break\n",
        "\n",
        "# Release the video and close all OpenCV windows\n",
        "video.release()\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "9WS9QNgtq9y0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# 모델과 라벨 로드\n",
        "model = load_model(\"/content/keras_model.h5\", compile=False)\n",
        "class_names = open(\"/content/labels.txt\", \"r\").readlines()\n",
        "\n",
        "# 비디오 파일 로드 (여러분의 비디오 파일 경로로 대체)\n",
        "video_path = \"/content/your_video.mp4\"\n",
        "video = cv2.VideoCapture(video_path)\n",
        "\n",
        "# 비디오의 FPS(초당 프레임 수)를 확인하여 처리할 프레임 수 계산\n",
        "fps = int(video.get(cv2.CAP_PROP_FPS))\n",
        "total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "duration = total_frames / fps\n",
        "\n",
        "print(f\"FPS: {fps}\")\n",
        "print(f\"Total Frames: {total_frames}\")\n",
        "print(f\"Duration: {duration}s\")\n",
        "\n",
        "# 비디오 처리\n",
        "while video.isOpened():\n",
        "    ret, frame = video.read()\n",
        "\n",
        "    # 프레임을 성공적으로 읽었다면\n",
        "    if ret:\n",
        "        # 프레임 크기 조정 및 전처리\n",
        "        frame = cv2.resize(frame, (224, 224))\n",
        "        frame = np.expand_dims(frame, axis=0)\n",
        "        frame = (frame / 127.5) - 1\n",
        "\n",
        "        # 모델 예측\n",
        "        predictions = model.predict(frame)\n",
        "        predicted_class = np.argmax(predictions[0])\n",
        "        confidence = np.max(predictions[0])\n",
        "\n",
        "        # 결과 출력\n",
        "        print(f\"Predicted Class: {class_names[predicted_class].strip()}, Confidence: {confidence:.2f}\")\n",
        "    else:\n",
        "        break\n",
        "\n",
        "# 자원 해제\n",
        "video.release()\n"
      ],
      "metadata": {
        "id": "nLH8wfApsuSc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}